{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Title\n",
        "\n",
        "### HW5 @ DL Course, Dr. Soleymani\n",
        "\n",
        "*Full Name:* ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we demonstrate that discriminative models inherently contain powerful generative capabilities, \n",
        "challenging the fundamental distinction between discriminative and generative architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "Machine learning has traditionally relied on a fundamental\n",
        "dichotomy: discriminative models map inputs to semantic\n",
        "representations, while generative models synthesize data\n",
        "from learned latent spaces. This separation has driven remarkable progress, \n",
        "from GANs to diffusion models.\n",
        "However, these approaches require extensive training on\n",
        "large datasets, raising questions about whether such complex \n",
        "training procedures are fundamentally necessary for high-quality generation.\n",
        "\n",
        "\n",
        "We will now demonstrate how discriminative models implicitly encode rich generative knowledge that can be accessed through careful optimization. Discriminative models excel at mapping images to representations $(f : I \\rightarrow v)$, but how can we reverse this process $(f^{-1} : v \\rightarrow I)$?In other words, \n",
        "> we are given a text $T$ and aim to find an image $I$ that maximizes the score between $T$ and $I$ ($S = \\text{score}(T, I)$). How can we determine that $I$? \n",
        "\n",
        "(Take a moment to think about this before continuing.)\n",
        "\n",
        "A simple solution is to use gradient ascent, where we compute the gradient of $S$ with respect to $I$ and update $I$ accordingly. Note that we are not updating the model's weights, but rather optimizing the input image.\n",
        "\n",
        "Before experimenting this proposed solution, we need to load a pre-trained CLIP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjols9DOq09s"
      },
      "source": [
        "## Basic setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAQMk62Ys_yI",
        "outputId": "073e9701-cff7-4850-94d7-ea27a3fd047e"
      },
      "outputs": [],
      "source": [
        "! pip install open_clip_torch\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdy-fcp0q9zV"
      },
      "source": [
        "## Getting the CLIP models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import SGD\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g4O9pxI02LP"
      },
      "outputs": [],
      "source": [
        "class CLIPModel:\n",
        "    \"\"\"\n",
        "    A wrapper class for a CLIP model, handling text and image encoding,\n",
        "    as well as image normalization.\n",
        "    \n",
        "    Attributes:\n",
        "        model: The CLIP model used for encoding.\n",
        "        tokenizer: Tokenizer for processing text inputs.\n",
        "        mean (Tensor): Mean values for image normalization.\n",
        "        std (Tensor): Standard deviation values for image normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, preprocess, device='cuda'):\n",
        "        \"\"\"\n",
        "        Initializes the CLIPModel with the given model, tokenizer, and preprocessing pipeline.\n",
        "        \n",
        "        Args:\n",
        "            model: The CLIP model instance.\n",
        "            tokenizer: Tokenizer for processing text inputs.\n",
        "            preprocess: Preprocessing pipeline containing transformations for images.\n",
        "            device (str, optional): The device to run the model on (default is 'cuda').\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mean = torch.Tensor(preprocess.transforms[-1].mean) \\\n",
        "                         .reshape([1, 3, 1, 1]) \\\n",
        "                         .to(device)\n",
        "        self.std = torch.Tensor(preprocess.transforms[-1].std) \\\n",
        "                        .reshape([1, 3, 1, 1]) \\\n",
        "                        .to(device)\n",
        "\n",
        "    def encode_text(self, texts):\n",
        "        \"\"\"\n",
        "        Encodes input text into an embedding using the CLIP model.\n",
        "        \n",
        "        Args:\n",
        "            texts (list or str): A list of text strings or a single text string.\n",
        "                - Shape: (batch_size) (list of strings)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: Encoded text embeddings.\n",
        "                - Shape: (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        tokenized_text = self.tokenizer(texts).to(device)\n",
        "        text_embeddings = self.model.encode_text(tokenized_text)\n",
        "        return text_embeddings\n",
        "\n",
        "    def encode_image(self, images):\n",
        "        \"\"\"\n",
        "        Encodes input images into embeddings using the CLIP model after normalization.\n",
        "        \n",
        "        Args:\n",
        "            images (Tensor): A batch of images as tensors.\n",
        "                - Shape: (batch_size, 3, height, width)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: Encoded image embeddings.\n",
        "                - Shape: (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        normalized_images = self.normalize_image(images)\n",
        "        image_embeddings = self.model.encode_image(normalized_images)\n",
        "        return image_embeddings\n",
        "\n",
        "    def normalize_image(self, x):\n",
        "        \"\"\"\n",
        "        Normalizes images.\n",
        "        \n",
        "        Args:\n",
        "            x (Tensor): Input image tensor.\n",
        "                - Shape: (batch_size, 3, height, width)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: Normalized image tensor.\n",
        "                - Shape: (batch_size, 3, height, width)\n",
        "        \"\"\"\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Sets the model to evaluation mode.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        \n",
        "def img_show(img):\n",
        "    \"\"\"\n",
        "    Display an image.\n",
        "    \n",
        "    Args:\n",
        "        img (Tensor): The image tensor to display.\n",
        "            - Shape: (channels, height, width)\n",
        "    \"\"\"\n",
        "    img = img.permute(1, 2, 0)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "8e0c0764abfa4a59a64579547c3e8c87",
            "aa4e4dcf437747508acbf72cb7216ffb",
            "61381004feae4e68b3d097a74cb2819b",
            "071801af2b264efc9b22d653bc730094",
            "016ab4bb16bb47369245f10cca00ec87",
            "968ce975bbf14083b8d8f4c4be91c18c",
            "5edb5938649c41669c487249ab12f919",
            "a8b8c8414fda4ac1a790ea6735929f83",
            "d6a74027452b4a01bedc6c0cb8688de9",
            "08995978c3a44c8aaea7a9bac50c26c5",
            "1d8db8bd1653443196d9b7f6b1470027",
            "2392c705f88748a6b5ba9e548992446e",
            "ca270fdf703847b8997837d20f68ad82",
            "a2f06a24add440dca8a8a620b1d10872",
            "ea438ecd51a24ee5aa86d0b68f3bf051",
            "70215a2430ae4ffb9a74990db4fc9566",
            "bd70f0d9db9242319348b78ed0b39506",
            "64bd0e65b07243458ee099a691d276f6",
            "45318a96997f498c9253f16e4ead13d9",
            "9b38889d45fc42688aac0a547f42d9c6",
            "1c32f61d6bf64717bc08869fdb7cc721",
            "0692432419e84411838cecb988d23f1f"
          ]
        },
        "id": "LcwZNhDqtjRw",
        "outputId": "51d67627-a1c9-4d9d-a57b-7be983ff5d1b"
      },
      "outputs": [],
      "source": [
        "models_to_load = [\n",
        "    (\"ViT-B-32\", \"laion400m_e32\"),\n",
        "    (\"ViT-B-32\", \"laion2b_s34b_b79k\"),\n",
        "    (\"OpenAI-ViT-B/32\", None),\n",
        "]\n",
        "\n",
        "models = []\n",
        "for model_name, pretrained in models_to_load:\n",
        "    if pretrained is not None:  # openclip models\n",
        "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "            model_name=model_name,\n",
        "            pretrained=pretrained\n",
        "        )\n",
        "\n",
        "        tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "    else:  # clip models\n",
        "        model, preprocess = clip.load(model_name.split(\"OpenAI-\")[1])\n",
        "\n",
        "        tokenizer = clip.tokenize\n",
        "\n",
        "    print(f\"Loaded {model_name} {preprocess}\")\n",
        "    model.to(device)\n",
        "    models.append(CLIPModel(model, tokenizer, preprocess))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing an Image for a Target Representation\n",
        "\n",
        "Now, let's implement our solution by first defining a function to compute the similarity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def similarity_score(\n",
        "    image_embeddings,\n",
        "    text_embeddings,\n",
        "    text_weights=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes the similarity score between image and text embeddings using cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        image_embeddings (Tensor): A tensor of image embeddings.\n",
        "            - Shape: (batch_size, embedding_dim)\n",
        "        text_embeddings (Tensor): A tensor of text embeddings.\n",
        "            - Shape: (num_texts, embedding_dim)\n",
        "        text_weights (list or Tensor, optional): A weight factor for text embeddings. Defaults to None.\n",
        "            - Shape: (num_texts) (if provided)\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: A tensor of similarity scores for each image.\n",
        "            - Shape: (batch_size)\n",
        "    \"\"\"\n",
        "    text_embeddings_normed = text_embeddings / \\\n",
        "        text_embeddings.norm(dim=-1, keepdim=True).float()\n",
        "\n",
        "    image_embeddings_normed = image_embeddings / \\\n",
        "        image_embeddings.norm(dim=-1, keepdim=True).float()\n",
        "\n",
        "    scores = image_embeddings_normed @ text_embeddings_normed.T\n",
        "\n",
        "    if text_weights is None:\n",
        "        return torch.mean(scores, axis=1)  # [images, texts]\n",
        "    else:\n",
        "        # [images, texts]\n",
        "        return torch.mean(scores * torch.Tensor(text_weights).to(device).reshape([1, -1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize_image(\n",
        "    model,\n",
        "    target_text,\n",
        "    image_size=224,\n",
        "    learning_rate=1e-1,\n",
        "    num_steps=100\n",
        "):\n",
        "    \"\"\"\n",
        "    Optimizes an image to match a given target text representation using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The pre-trained CLIP model used to encode images and text.\n",
        "        target_text (str): The target text description to guide the image optimization.\n",
        "        image_size (int, optional): The resolution of the generated image. Defaults to 224.\n",
        "        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 1e-1.\n",
        "        num_steps (int, optional): The number of optimization steps. Defaults to 100.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The optimized image tensor with shape [1, 3, image_size, image_size].\n",
        "    \"\"\"\n",
        "    input_shape = (1, 3, image_size, image_size)\n",
        "\n",
        "    image = torch.rand(input_shape, device=device, requires_grad=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam([image], lr=learning_rate)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        image_embedding = model.encode_image(image)\n",
        "        text_embedding = model.encode_text(target_text)\n",
        "        loss = -1.0 * similarity_score(\n",
        "            image_embeddings=image_embedding,\n",
        "            text_embeddings=text_embedding,\n",
        "        )\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        image.data = torch.clamp(image.data, 0, 1)\n",
        "\n",
        "        if (step + 1) % 10 == 0 or (step == 0):\n",
        "            print(f\"Step {step + 1}/{num_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_image = optimize_image(models[0], \"a photo of a cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resulting image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_show(generated_image[0].detach().cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, the model identifies the image as a cat—but is it really? Although the image achieves a high similarity score, it doesn’t truly resemble a cat. This optimized image, which tricks the model into making an incorrect prediction, is known as an **adversarial image**.  \n",
        "\n",
        "To better understand why this image was generated, take a look at the following illustration:\n",
        "\n",
        "![mapping-between-images-and-embeddings.png](https://i.postimg.cc/NG2ZbP9J/mapping-between-images-and-embeddings.png)\n",
        "\n",
        "A region of all images corresponding to a {text, image} embedding contains \n",
        "interpretable images as well as noise-like adversarial patterns.\n",
        "Reconstructing an image from an embedding typically leads to\n",
        "such a degenerate noisy image. \n",
        "\n",
        "We now need a method to ensure that the reconstructed image lies within the interpretable region of the manifold. Upon reviewing the generated image, you'll notice that it exhibits high-frequency patterns. In the following sections, we will modify the optimization process to avoid the degenerate high-frequency solutions commonly seen in adversarial examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Resolution Optimization\n",
        "We decompose the optimization across multiple scales – a choice that proves surprisingly powerful in guiding solutions toward natural images.\n",
        "We break the degeneracy by decomposing the optimization across multiple scales. Instead of directly optimizing pixels,\n",
        "we express the image as a sum of resolution components:\n",
        "$$\n",
        "I = \\frac{1}{2} + \\frac{1}{2} \\tanh \\left(\\sum_{r \\in \\rho} \\text{resize}_{224}(P_r)\\right)\n",
        "$$\n",
        "where $P_r \\in \\mathbb{R}^{r \\times r \\times 3}$ represents the image component at\n",
        "resolution $r$, and $\\rho$ spans from $1 \\times 1$ to $224 \\times 224$. The tanh\n",
        "transformation maps unbounded optimization values to valid\n",
        "pixel intensities while maintaining gradient flow.\n",
        "\n",
        "The optimization objective becomes:\n",
        "$$\n",
        "\\sum_{i,j} \\frac{\\partial \\text{score}_i(\\text{augment}_j(I(P_1, \\ldots, P_{224})))}{\\partial (P_1, \\ldots, P_{224})}\n",
        "$$\n",
        "\n",
        "where $i$ indexes multiple CLIP models and $j$ indexes augmentations. This formulation has several key properties: \n",
        "1. Components are optimized simultaneously across all resolu-\n",
        "tions \n",
        "2. Gradients naturally distribute across scales based\n",
        "on their importance \n",
        "3. High-frequency adversarial patterns are suppressed by scale decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpqmUNzyrA_v"
      },
      "source": [
        "## Image Converters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GV2a7wE-WS-"
      },
      "outputs": [],
      "source": [
        "def raw_to_real_image(raw_image):\n",
        "    \"\"\"\n",
        "    Converts a raw image tensor to a real image tensor by applying a hyperbolic tangent transformation\n",
        "    and scaling the result to the range [0, 1].\n",
        "\n",
        "    Args:\n",
        "        raw_image (Tensor): The raw image tensor to be converted.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The transformed real image tensor with values in the range [0, 1].\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "    \"\"\"\n",
        "    return 0.5 * torch.tanh(raw_image) + 0.5\n",
        "\n",
        "\n",
        "def real_to_raw_image(real_image, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Converts a real image tensor to a raw image tensor by clipping the real image values to a safe range\n",
        "    and applying the inverse hyperbolic tangent transformation.\n",
        "    \n",
        "    Args:\n",
        "        real_image (Tensor): The real image tensor to be converted.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "        eps (float, optional): A small value used to prevent instability near 0 or 1 during arctanh calculation.\n",
        "            Defaults to 1e-5.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The transformed raw image tensor with values in the range [-1, 1].\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "    \"\"\"\n",
        "    tanh_result = torch.clip(real_image, eps, 1-eps) * 2.0 - 1.0\n",
        "    return torch.arctanh(tanh_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2As7VDES8SDe"
      },
      "source": [
        "## Augmentation tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiJtsugVeMtL"
      },
      "outputs": [],
      "source": [
        "def add_jitter(x, size=3):\n",
        "    \"\"\"\n",
        "    Adds random jitter to an image by shifting it horizontally and vertically \n",
        "    within the specified range.\n",
        "    \n",
        "    Args:\n",
        "        x (Tensor): The input image tensor to which jitter will be applied.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "        size (int, optional): The maximum number of pixels to shift in either direction.\n",
        "            Default is 3.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The image tensor with random shifts applied.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "    \"\"\"\n",
        "    x_shift = np.random.choice(range(-size, size+1))\n",
        "    y_shift = np.random.choice(range(-size, size+1))\n",
        "\n",
        "    x = torch.roll(x, shifts=(x_shift, y_shift), dims=(-2, -1))\n",
        "    return x\n",
        "\n",
        "\n",
        "def center_crop(x, out_size=224):\n",
        "    \"\"\"\n",
        "    Crops the central region of an image tensor to a specified output size.\n",
        "    \n",
        "    Args:\n",
        "        x (Tensor): The input image tensor to be cropped.\n",
        "            - Shape: [batch_size, channels, height, width]\n",
        "        out_size (int, optional): The target output size for both height and width.\n",
        "            Default is 224.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The cropped image tensor with the specified output size.\n",
        "            - Shape: [batch_size, channels, out_size, out_size]\n",
        "    \"\"\"\n",
        "    in_size = x.shape[2]\n",
        "    begin = (in_size - out_size) // 2\n",
        "    end = begin + out_size\n",
        "\n",
        "    x = x[:, :, begin:end, begin:end]\n",
        "    return x\n",
        "\n",
        "\n",
        "def add_noise(x, scale=0.1):\n",
        "    \"\"\"\n",
        "    Adds random noise to an image tensor by adding uniform noise scaled by the specified factor.\n",
        "    \n",
        "    Args:\n",
        "        x (Tensor): The input image tensor to which noise will be added.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "        scale (float, optional): The scaling factor for the noise. Default is 0.1.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The noisy image tensor.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "    \"\"\"\n",
        "    return x + scale * torch.rand_like(x)\n",
        "\n",
        "\n",
        "def augment_image(\n",
        "    image_in: torch.Tensor,\n",
        "    count: int = 1,\n",
        "    jitter_scale: float = 3,\n",
        "    noise_scale: float = 0.1,\n",
        "    clip: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs multiple augmentations on an input image, including jitter, center cropping, and noise addition.\n",
        "    The function can generate multiple augmented images if requested.\n",
        "    \n",
        "    Args:\n",
        "        image_in (Tensor): The input image tensor to be augmented.\n",
        "            - Shape: (batch_size, channels, height, width)\n",
        "        count (int, optional): The number of augmented images to generate. Default is 1.\n",
        "        jitter_scale (float, optional): The scale of random jitter to apply. Default is 3.\n",
        "        noise_scale (float, optional): The scale of noise to add. Default is 0.1.\n",
        "        clip (bool, optional): Whether to clip the values of the resulting image to the range [0, 1]. Default is True.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The augmented image tensor with the specified number of augmented images.\n",
        "            - Shape: (count * batch_size, channels, out_size, out_size)\n",
        "    \"\"\"\n",
        "\n",
        "    augmented_images = []\n",
        "    for _ in range(count):\n",
        "        augmented_img = add_jitter(image_in, size=jitter_scale)\n",
        "        augmented_img = center_crop(augmented_img)\n",
        "        augmented_img = add_noise(augmented_img, scale=noise_scale)\n",
        "\n",
        "        augmented_images.append(augmented_img)\n",
        "\n",
        "    result = torch.cat(augmented_images, dim=0)\n",
        "    return torch.clamp(result, 0, 1) if clip else result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGaglVeR-gdu"
      },
      "source": [
        "## Image Generation using Multi-Resolution Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sk7tV4in9gq"
      },
      "outputs": [],
      "source": [
        "def generate_image(\n",
        "    models,\n",
        "    text_weight_pairs,\n",
        "    source_image=None,\n",
        "    original_resolution=224,\n",
        "    large_resolution=224 + 2 * 56,  # adding the buffer on the side\n",
        "    resolutions=range(1, 336 + 1),\n",
        "    batch_size=32,\n",
        "    lr=2e-1,\n",
        "    steps=100,\n",
        "    jitter_scale=56,\n",
        "    noise_scale=0.2,\n",
        "    augmentation_copies=32,  # how many augmentations to get a gradient from at once\n",
        "    step_to_show=10,  # how often to show the image during generation\n",
        "    num_generations=1,\n",
        "    guiding_images=None,\n",
        "    image_weights=None,\n",
        "    inpainting_mask=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates an image by optimizing the input image using a set of models and target text-image pairs.\n",
        "    \n",
        "    Args:\n",
        "        models (list): A list of models to use for image generation.\n",
        "        text_weight_pairs (list of tuples): A list of (weight, text) pairs for guiding the image generation.\n",
        "        source_image (Tensor, optional): The starting image for generation. If None, a random image is used.\n",
        "            - Shape: (num_sources, channels, height, width)\n",
        "        original_resolution (int, optional): The original resolution of the image. Default is 224.\n",
        "        large_resolution (int, optional): The resolution of the image, including the buffer. Default is 336.\n",
        "        resolutions (range or list, optional): A range or list of resolutions to generate. Default is range(1, 337).\n",
        "        batch_size (int, optional): The batch size used for optimization. Default is 32.\n",
        "        lr (float, optional): The learning rate for optimization. Default is 2e-1.\n",
        "        steps (int, optional): The number of optimization steps. Default is 100.\n",
        "        jitter_scale (int, optional): The scale of jitter applied to the images during augmentation. Default is 56.\n",
        "        noise_scale (float, optional): The scale of noise added to the images during augmentation. Default is 0.2.\n",
        "        augmentation_copies (int, optional): The number of augmentations to create at once for gradient updates. Default is 32.\n",
        "        step_to_show (int, optional): The frequency of steps at which to display the generated image. Default is 10.\n",
        "        num_generations (int, optional): The number of images to generate. Default is 1.\n",
        "        guiding_images (Tensor, optional): Additional guiding images for the generation process.\n",
        "            - Shape: (num_guiding_images, channels, height, width)\n",
        "        image_weights (list, optional): Weights for the guiding images.\n",
        "        inpainting_mask (Tensor, optional): A mask used for inpainting during the generation process.\n",
        "            - Shape: (num_generations, channels, height, width)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - collected_images (list): A list of images generated at each step.\n",
        "            - components (list): The component images for each resolution.\n",
        "    \"\"\"\n",
        "    target_texts = [x[1] for x in text_weight_pairs]\n",
        "    target_weights = [x[0] for x in text_weight_pairs]\n",
        "\n",
        "    # just making sure no duplicates and sorted\n",
        "    resolutions = sorted(set(resolutions))\n",
        "    print(f\"Resolutions = {resolutions}\")\n",
        "\n",
        "    guiding_embeddings, guiding_weights = get_guidance(\n",
        "        models, guiding_images, image_weights, target_texts, target_weights)\n",
        "\n",
        "    starting_image = get_starting_image(\n",
        "        source_image, large_resolution, num_generations)\n",
        "\n",
        "    batch_count = int(np.ceil(augmentation_copies / batch_size))\n",
        "\n",
        "    raw_starting_image = real_to_raw_image(starting_image)\n",
        "\n",
        "    components = [torch.zeros(\n",
        "        num_generations, 3, res, res, requires_grad=True, device=device) for res in resolutions]\n",
        "\n",
        "    optimizer = SGD(components, lr=lr)\n",
        "\n",
        "    # Scheduler setup -- just constant works well!\n",
        "    scheduler = LambdaLR(optimizer, lambda step: 1.0)\n",
        "\n",
        "    collected_images = []\n",
        "    for step in tqdm.tqdm(range(steps)):\n",
        "        components_sum = sum([F.interpolate(p, size=(\n",
        "            large_resolution, large_resolution), mode='bicubic') for p in components])\n",
        "\n",
        "        if inpainting_mask is not None:\n",
        "            components_sum.register_hook(\n",
        "                lambda grad: grad * inpainting_mask if inpainting_mask is not None else grad)\n",
        "\n",
        "        losses = []\n",
        "        for it in range(batch_count):\n",
        "            image_count = min(\n",
        "                [batch_size, augmentation_copies - (it * batch_size)])\n",
        "\n",
        "            for model, model_guiding_embeddings in zip(models, guiding_embeddings):\n",
        "                model.eval()\n",
        "\n",
        "                input_image = raw_to_real_image(raw_starting_image + components_sum)\n",
        "\n",
        "                image_variations = augment_image(\n",
        "                    input_image,\n",
        "                    image_count,\n",
        "                    jitter_scale=jitter_scale,\n",
        "                    noise_scale=noise_scale,\n",
        "                )\n",
        "\n",
        "                image_embeddings = model.encode_image(image_variations)\n",
        "                loss = -1.0 * similarity_score(\n",
        "                    image_embeddings=image_embeddings,\n",
        "                    text_embeddings=model_guiding_embeddings,\n",
        "                    text_weights=guiding_weights,\n",
        "                )\n",
        "\n",
        "                loss = torch.mean(loss) * num_generations\n",
        "                losses.append(loss.item())\n",
        "                loss.backward(retain_graph=True)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        collected_images.append(\n",
        "            raw_to_real_image(raw_starting_image + components_sum).detach().cpu()\n",
        "        )\n",
        "\n",
        "        ell_infty = torch.max(\n",
        "            torch.abs(collected_images[-1][0]-collected_images[0][0])) * 255\n",
        "\n",
        "        if step % step_to_show == 0:\n",
        "            print(f\"step={step} {[round(x, 3) for x in losses]} ell_infty={ell_infty:.0f}/255\")\n",
        "            plot_collected_images(num_generations, collected_images)\n",
        "\n",
        "    return collected_images, components\n",
        "\n",
        "\n",
        "def get_starting_image(source_image, large_resolution, num_generations):\n",
        "    \"\"\"\n",
        "    Returns the starting image for the generation process.\n",
        "    \n",
        "    Args:\n",
        "        source_image (Tensor, optional): The initial image to use as the starting point.\n",
        "            - Shape: (num_sources, channels, height, width]\n",
        "        large_resolution (int): The resolution of the output image.\n",
        "        num_generations (int): The number of images to generate.\n",
        "    \n",
        "    Returns:\n",
        "        Tensor: The starting image tensor, possibly repeated for multiple generations.\n",
        "            - Shape: (num_generations, channels, large_resolution, large_resolution)\n",
        "    \"\"\"\n",
        "    if source_image is None:\n",
        "        original_image = torch.full(\n",
        "            (num_generations, 3, large_resolution, large_resolution), 0.5).to(device)\n",
        "    else:\n",
        "        original_image = torch.concatenate(\n",
        "            [source_image] * num_generations, axis=0).to(device)\n",
        "\n",
        "    return original_image\n",
        "\n",
        "\n",
        "def plot_collected_images(num_generations, collected_images):\n",
        "    \"\"\"\n",
        "    Plots the collected images during the generation process.\n",
        "\n",
        "    Args:\n",
        "        num_generations (int): The number of generated images to display.\n",
        "        collected_images (list): A list of images generated at each step.\n",
        "            Each entry in the list is a tensor of shape (num_generations, channels, height, width).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(3 * 5.5, num_generations * 5), dpi=75)\n",
        "    for i in range(num_generations):\n",
        "        plt.subplot(num_generations, 3, 1 + i * 3)\n",
        "        img_show(collected_images[0][i])\n",
        "        plt.title(\"Source image\")\n",
        "\n",
        "        plt.subplot(num_generations, 3, 2 + i * 3)\n",
        "        img_show(collected_images[-1][i])\n",
        "        plt.title(\"Generated image\")\n",
        "\n",
        "        plt.subplot(num_generations, 3, 3 + i * 3)\n",
        "        img_show(\n",
        "            0.5 + (collected_images[-1][i] - collected_images[0][i]))\n",
        "        plt.title(\"Residual image\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_guidance(models, guiding_images, image_weights, target_texts, target_weights):\n",
        "    \"\"\"\n",
        "    Computes the guidance embeddings (text and image) for the generation process.\n",
        "    \n",
        "    Args:\n",
        "        models (list): A list of models used to encode the text and images.\n",
        "        guiding_images (Tensor, optional): Additional guiding images used for the generation.\n",
        "            - Shape: (num_guiding_images, channels, height, width)\n",
        "        image_weights (list, optional): Weights for the guiding images.\n",
        "        target_texts (list of str): The target texts for the generation.\n",
        "        target_weights (list of float): The weights for each target text.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - guiding_embeddings (Tensor): The combined text and image embeddings.\n",
        "                Shape: (num_models, num_guidings, embeddings_dim)\n",
        "            - guiding_weights (list): The weights for each guidance.\n",
        "    \"\"\"\n",
        "    text_embeddings = torch.stack(\n",
        "        [model.encode_text(target_texts).detach() for model in models])  # (#models, batch, features)\n",
        "\n",
        "    if guiding_images is None:\n",
        "        return text_embeddings, target_weights\n",
        "    else:\n",
        "        guiding_image_embeddings = torch.stack(\n",
        "            [model.encode_image(guiding_images.to(device)).detach() for model in models])\n",
        "\n",
        "        guiding_embeddings = torch.concatenate(\n",
        "            [text_embeddings, guiding_image_embeddings], axis=1)\n",
        "\n",
        "        if image_weights is None:\n",
        "            image_weights = [1.0] * guiding_images.shape[0]\n",
        "\n",
        "        guiding_weights = target_weights + image_weights\n",
        "\n",
        "        return guiding_embeddings, guiding_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r21pWO8wrWyD"
      },
      "source": [
        "## Chosing the models to use in the ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFJ2j2w8CKKv"
      },
      "outputs": [],
      "source": [
        "# fixing the model selection\n",
        "chosen_model_ids = [0, 1, 2]\n",
        "models = [x for i, x in enumerate(models) if i in chosen_model_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_GBYth8rixt"
      },
      "source": [
        "# Plotting and analysis tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DphdWhAsuBbP"
      },
      "outputs": [],
      "source": [
        "def display_images(\n",
        "    collected_images,\n",
        "    original_res=224,\n",
        "    large_res=336,\n",
        "    guiding_image=None,\n",
        "    starting_image=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Displays the starting image, guiding image, and generated images side by side.\n",
        "    \n",
        "    Args:\n",
        "        collected_images (list of Tensor): A list of tensors representing the generated images at each step.\n",
        "            Each tensor should have the shape (num_generations, channels, height, width).\n",
        "        original_res (int, optional): The resolution of the images to display. Default is 224.\n",
        "        large_res (int, optional): The resolution of the images, including the buffer. Default is 336.\n",
        "        guiding_image (Tensor, optional): The guiding image used during the generation process.\n",
        "            - Shape: (channels, height, width)\n",
        "        starting_image (Tensor, optional): The starting image used for generation.\n",
        "            - Shape: (channels, height, width)\n",
        "    \"\"\"\n",
        "    num_images = collected_images[-1].shape[0] + (guiding_image != None) + (starting_image != None)\n",
        "\n",
        "    plt.figure(figsize=(num_images * 224 / 100, 224 / 100), dpi=112)\n",
        "    subplot_idx = 1\n",
        "\n",
        "    begin = (large_res - original_res) // 2\n",
        "    end = begin + original_res\n",
        "\n",
        "    if starting_image is not None:\n",
        "        plt.subplot(1, num_images, subplot_idx)\n",
        "        img_show(starting_image[0].detach().cpu())\n",
        "        plt.title(\"Starting image\", fontsize=8)\n",
        "        subplot_idx += 1\n",
        "\n",
        "    if guiding_image is not None:\n",
        "        plt.subplot(1, num_images, subplot_idx)\n",
        "        img_show(guiding_image[0].detach().cpu())\n",
        "        plt.title(\"Guiding image\", fontsize=8)\n",
        "        subplot_idx += 1\n",
        "\n",
        "    for i, image in enumerate(collected_images[-1]):\n",
        "        plt.subplot(1, num_images, subplot_idx)\n",
        "        img_show(image[:, begin:end, begin:end].detach().cpu())\n",
        "        if len(collected_images[-1]) == 1:\n",
        "            plt.title(f\"Generated image\", fontsize=8)\n",
        "        else:\n",
        "            plt.title(f\"Generated image v{i + 1}\", fontsize=8)\n",
        "        subplot_idx += 1\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ag23xS0_jOn"
      },
      "outputs": [],
      "source": [
        "def visualize_individual_resolutions(\n",
        "    components,\n",
        "    version_i=0,\n",
        "    selected_resolutions=[1, 2, 4, 8, 16, 32, 64],\n",
        "    large_resolution=336,\n",
        "\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualizes the components of a generated image at different resolutions, both at their original resolution \n",
        "    and interpolated to a larger resolution. This helps to understand the contributions of different resolutions \n",
        "    to the overall generated image.\n",
        "\n",
        "    Args:\n",
        "        components (list of Tensor): A list of tensors representing the image components at various resolutions. \n",
        "            Each tensor is of shape [num_generations, channels, height, width], with different resolutions.\n",
        "        version_i (int, optional): The index for the version of the generated image to display. Defaults to 0.\n",
        "        selected_resolutions (list of int, optional): The list of resolutions to visualize. Each resolution in \n",
        "            the list should correspond to a downsampled version of the image. Default is [1, 2, 4, 8, 16, 32, 64].\n",
        "        large_resolution (int, optional): The resolution to which the components will be interpolated for comparison.\n",
        "            Default is 336.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(3 * len(selected_resolutions), 3 * 2))\n",
        "    for component in components:\n",
        "        if component.shape[2] in selected_resolutions:\n",
        "\n",
        "            plt.subplot(2, len(selected_resolutions), selected_resolutions.index(\n",
        "                component.shape[2]) + 1)\n",
        "\n",
        "            data = component[version_i]\n",
        "            data = raw_to_real_image(data).detach(\n",
        "            ).cpu()\n",
        "            data = data - torch.min(data)\n",
        "            data = data / torch.max(data)\n",
        "            img_show(data)\n",
        "            plt.title(f\"r = {component.shape[2]}\")\n",
        "\n",
        "            plt.subplot(2, len(selected_resolutions), selected_resolutions.index(\n",
        "                component.shape[2])+1+len(selected_resolutions))\n",
        "\n",
        "            data_interpolated = F.interpolate(component, size=(\n",
        "                large_resolution, large_resolution), mode='bicubic')\n",
        "\n",
        "            data = raw_to_real_image(data_interpolated[version_i]).detach(\n",
        "            ).cpu()\n",
        "            data = data - torch.min(data)\n",
        "            data = data / torch.max(data)\n",
        "            img_show(data)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168jj91PqZAI"
      },
      "source": [
        "# Task 1 = Image generation from text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCgpY-QZmdYb"
      },
      "outputs": [],
      "source": [
        "text_weight_pairs = [\n",
        "    (1.0, \"a photo of swiss mountain valley\"),\n",
        "    (-0.3, 'cloudy'),\n",
        "    (0.3, \"several homes, lush forest, rivers\"),\n",
        "    (0.3, \"humans, peoples\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yk4i810opGpt",
        "outputId": "a80d19bb-2f66-4339-9dc6-fa8a421e3d2e"
      },
      "outputs": [],
      "source": [
        "collected_images, components = generate_image(\n",
        "    models,\n",
        "    text_weight_pairs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaFgJ1BHrzTz"
      },
      "source": [
        "## Resulting image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "28H-0ibG93oO",
        "outputId": "37a5ccca-fdd2-4b61-bcaf-64f4a45001ce"
      },
      "outputs": [],
      "source": [
        "display_images(\n",
        "    collected_images,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY7m3SgsB3zt"
      },
      "source": [
        "### Individual resolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "GYKX59xV-Otn",
        "outputId": "f5cc0150-08c5-4109-c598-271d7ca2b3d1"
      },
      "outputs": [],
      "source": [
        "visualize_individual_resolutions(components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh2IWHGCB8Il"
      },
      "source": [
        "# Task 2 = Generation stability over multiple runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGFGDEDVCOIy"
      },
      "outputs": [],
      "source": [
        "text_weight_pairs = [\n",
        "  (1.0, \"a beautiful photo of  Mount Pinatubo eruption., detailed\"),\n",
        "  (-0.3,\"obscured crater\"),\n",
        "  (0.3, \"highly realistic\"),\n",
        "  (-0.3, \"multiple exposure\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JQqdS5HkCOIz",
        "outputId": "149e7846-6b62-4bad-fccc-24c3f3615de2"
      },
      "outputs": [],
      "source": [
        "collected_images, components = generate_image(\n",
        "  models,\n",
        "  text_weight_pairs,\n",
        "  num_generations=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejgxkl-pr_xM"
      },
      "source": [
        "## Resulting images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "VPCxx8kECWAg",
        "outputId": "2d8d7f34-936a-4a38-e5c4-6a27c3540411"
      },
      "outputs": [],
      "source": [
        "display_images(\n",
        "    collected_images,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odXtzDaXj_OP"
      },
      "source": [
        "# Task 3 = \"Style\" transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXB8HOKjCzbs"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(224),  # Resize shorter side to 224, maintain aspect ratio\n",
        "            transforms.CenterCrop(224),  # Crop the center 224x224\n",
        "            transforms.ToTensor(),  # Convert to tensor and normalize to [0,1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        tensor = self.transform(image)\n",
        "        return tensor\n",
        "\n",
        "def load_images(image_paths, batch_size=32):\n",
        "    \"\"\"\n",
        "    Load images from list of paths and return batched tensor\n",
        "\n",
        "    Args:\n",
        "        image_paths (list): List of paths to image files\n",
        "        batch_size (int): Size of batches to return\n",
        "\n",
        "    Returns:\n",
        "        DataLoader that yields tensors of shape [batch_size, 3, 224, 224]\n",
        "    \"\"\"\n",
        "    dataset = ImageDataset(image_paths)\n",
        "    dataloader = DataLoader(dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVG_4FO6kp3f"
      },
      "source": [
        "### Starting image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkvZmtYJkl24",
        "outputId": "632ce270-8b54-4926-d40a-5332cdbed152"
      },
      "outputs": [],
      "source": [
        "!wget https://www.noesnest.com/wp-content/uploads/sites/14/2020/03/san-francisco-at-night.jpg -O start_image.jpg\n",
        "\n",
        "image_paths = [\n",
        "    \"start_image.jpg\"\n",
        "]\n",
        "\n",
        "loader = load_images(image_paths)\n",
        "\n",
        "batch = next(iter(loader))\n",
        "starting_image = batch\n",
        "\n",
        "large_res = 336\n",
        "\n",
        "eps = 0.1 # to offset the image from the ends of the brigthness range\n",
        "starting_image = starting_image * (1-2*eps) + eps\n",
        "\n",
        "# adding padding to make it 336 size in total (not resizing, padding)\n",
        "starting_image = F.pad(starting_image, ((large_res-starting_image.shape[2])//2, (large_res-starting_image.shape[2])//2, (large_res-starting_image.shape[3])//2, (large_res-starting_image.shape[3])//2), \"constant\", 0.5)#1-eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "tzzgZghmkz9Z",
        "outputId": "95357620-9318-4c26-f020-9d612479ecb4"
      },
      "outputs": [],
      "source": [
        "offset = (large_res - 224) // 2\n",
        "plt.figure(figsize=(4 * starting_image.shape[0], 4))\n",
        "for i in range(starting_image.shape[0]):\n",
        "    plt.subplot(1, starting_image.shape[0], i + 1)\n",
        "    img_show(starting_image[i][:, offset:-offset, offset:-offset])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An_WWq4Nl5YA"
      },
      "source": [
        "### \"Style\" guiding image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlGjbtJnlXLL",
        "outputId": "751e9979-6b5f-4eb2-9f8b-f341a8be31ba"
      },
      "outputs": [],
      "source": [
        "# !wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/970px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -O guiding_image1.png\n",
        "\n",
        "!wget https://i.postimg.cc/44hZvX9K/majestic-medieval-castle-stockcake.jpg -O guiding_image1.png\n",
        "image_paths = [\n",
        "    \"guiding_image1.png\"\n",
        "]\n",
        "\n",
        "loader = load_images(image_paths)\n",
        "batch = next(iter(loader))\n",
        "guiding_image = batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "aty7SZz-mF_j",
        "outputId": "05f0fbc9-a80a-42b0-d103-ab3d0926db1e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4 * guiding_image.shape[0], 4))\n",
        "for i in range(guiding_image.shape[0]):\n",
        "    plt.subplot(1, guiding_image.shape[0], i + 1)\n",
        "    img_show(guiding_image[i])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wsgMLASmWwN"
      },
      "source": [
        "### Combining the two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A6JZYfbImRlo",
        "outputId": "b6423e9a-153d-4b50-f158-4c9a1aa77f92"
      },
      "outputs": [],
      "source": [
        "collected_images, components = generate_image(\n",
        "  models,\n",
        "  text_weight_pairs = [],\n",
        "  guiding_images = guiding_image,\n",
        "  source_image = starting_image.detach().cpu(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3GP0Pw-sEVX"
      },
      "source": [
        "## Resulting image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "8gFD4li2mc2O",
        "outputId": "42bb0faa-6b64-42c5-e1cb-fa2a25cc0dd9"
      },
      "outputs": [],
      "source": [
        "display_images(\n",
        "    collected_images,\n",
        "    guiding_image = guiding_image,\n",
        "    starting_image = starting_image\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWtAYXrtnICK"
      },
      "source": [
        "# Task 4 = Reconstructing an image from its embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCUYRo-6nPNl"
      },
      "source": [
        "## Getting the image to reconstruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-_W5NgAnLBq",
        "outputId": "eec7900f-7174-479d-8f4b-29bee79082f6"
      },
      "outputs": [],
      "source": [
        "# !wget https://www.rmg.co.uk/sites/default/files/styles/full_width_1440/public/Henry%20VIII.jpg -O guiding_image1.png\n",
        "!wget https://i.postimg.cc/1XX3ZdR8/Screenshot-from-2025-04-02-11-26-06.png -O guiding_image1.png\n",
        "image_paths = [\n",
        "    \"guiding_image1.png\"\n",
        "]\n",
        "\n",
        "loader = load_images(image_paths)\n",
        "batch = next(iter(loader))\n",
        "guiding_image = batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "WYviWN2teMtU",
        "outputId": "ea85e5d0-98c0-4b67-8ade-7e8bb1e0d6c7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4 * guiding_image.shape[0], 4))\n",
        "for i in range(guiding_image.shape[0]):\n",
        "    plt.subplot(1, guiding_image.shape[0], i + 1)\n",
        "    img_show(guiding_image[i])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4EZEN2hoLhO"
      },
      "source": [
        "## Reconstruction the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4icwX0T0nTqy",
        "outputId": "e9574a70-f859-43d1-cb1f-66755ca4f6d7"
      },
      "outputs": [],
      "source": [
        "# helping the reconstruction with a content agnostic set of prompts\n",
        "text_weight_pairs = [\n",
        "  (0.6,\"cohesive single subject\"),\n",
        "  (-0.6, \"multiple exposure\"),\n",
        "]\n",
        "\n",
        "collected_images, _ = generate_image(\n",
        "  models,\n",
        "  text_weight_pairs = text_weight_pairs,\n",
        "  guiding_images = guiding_image,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ce7giYsG0i"
      },
      "source": [
        "## Resulting image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "NfZCvaymn-cv",
        "outputId": "800283c1-2361-409a-bfaf-cc0f1bb6a784"
      },
      "outputs": [],
      "source": [
        "display_images(\n",
        "    collected_images,\n",
        "    guiding_image = guiding_image,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p40BbwlfI29",
        "outputId": "f7fda286-aa10-4679-e723-4a9faa828ad3"
      },
      "outputs": [],
      "source": [
        "!wget https://www.noesnest.com/wp-content/uploads/sites/14/2020/03/san-francisco-at-night.jpg -O start_image.jpg\n",
        "\n",
        "image_paths = [\n",
        "    \"start_image.jpg\"\n",
        "]\n",
        "\n",
        "loader = load_images(image_paths)\n",
        "\n",
        "batch = next(iter(loader))\n",
        "starting_image = batch\n",
        "\n",
        "large_res = 336\n",
        "\n",
        "eps = 0.1 # to offset the image from the ends of the brigthness range\n",
        "starting_image = starting_image * (1-2*eps) + eps\n",
        "\n",
        "# adding padding to make it 336 size in total (not resizing, padding)\n",
        "starting_image = F.pad(starting_image, ((large_res-starting_image.shape[2])//2, (large_res-starting_image.shape[2])//2, (large_res-starting_image.shape[3])//2, (large_res-starting_image.shape[3])//2), \"constant\", 0.5)#1-eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeAFtbY6fke4"
      },
      "outputs": [],
      "source": [
        "text_weight_pairs = [\n",
        "    (1.0, \"a vast night sky filled with countless twinkling stars\"),\n",
        "    (0.3, \"the stars vary in size and brightness\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "33nrtI0rmbDv",
        "outputId": "702a4e3b-d36c-4755-a793-e83f996cf2be"
      },
      "outputs": [],
      "source": [
        "img_show(starting_image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "DYk0KnIFfvYQ",
        "outputId": "2ac3642f-2864-48c8-f67f-3f1dd5c875b8"
      },
      "outputs": [],
      "source": [
        "mask = torch.zeros_like(starting_image)\n",
        "\n",
        "mask[:, :, 60:120,70:270] = 1\n",
        "\n",
        "img_show((starting_image * (1 - mask))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x2Z7W6SlfgN1",
        "outputId": "d5410aeb-7fe4-4932-bccb-672d310c3f14"
      },
      "outputs": [],
      "source": [
        "collected_images, components = generate_image(\n",
        "    models,\n",
        "    text_weight_pairs,\n",
        "    inpainting_mask=mask.to(device),\n",
        "    source_image = starting_image.detach().cpu(),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "016ab4bb16bb47369245f10cca00ec87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0692432419e84411838cecb988d23f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "071801af2b264efc9b22d653bc730094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08995978c3a44c8aaea7a9bac50c26c5",
            "placeholder": "​",
            "style": "IPY_MODEL_1d8db8bd1653443196d9b7f6b1470027",
            "value": " 605M/605M [00:03&lt;00:00, 178MB/s]"
          }
        },
        "08995978c3a44c8aaea7a9bac50c26c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c32f61d6bf64717bc08869fdb7cc721": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8db8bd1653443196d9b7f6b1470027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2392c705f88748a6b5ba9e548992446e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca270fdf703847b8997837d20f68ad82",
              "IPY_MODEL_a2f06a24add440dca8a8a620b1d10872",
              "IPY_MODEL_ea438ecd51a24ee5aa86d0b68f3bf051"
            ],
            "layout": "IPY_MODEL_70215a2430ae4ffb9a74990db4fc9566"
          }
        },
        "45318a96997f498c9253f16e4ead13d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5edb5938649c41669c487249ab12f919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61381004feae4e68b3d097a74cb2819b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b8c8414fda4ac1a790ea6735929f83",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6a74027452b4a01bedc6c0cb8688de9",
            "value": 605143284
          }
        },
        "64bd0e65b07243458ee099a691d276f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70215a2430ae4ffb9a74990db4fc9566": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e0c0764abfa4a59a64579547c3e8c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa4e4dcf437747508acbf72cb7216ffb",
              "IPY_MODEL_61381004feae4e68b3d097a74cb2819b",
              "IPY_MODEL_071801af2b264efc9b22d653bc730094"
            ],
            "layout": "IPY_MODEL_016ab4bb16bb47369245f10cca00ec87"
          }
        },
        "968ce975bbf14083b8d8f4c4be91c18c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b38889d45fc42688aac0a547f42d9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2f06a24add440dca8a8a620b1d10872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45318a96997f498c9253f16e4ead13d9",
            "max": 605143316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b38889d45fc42688aac0a547f42d9c6",
            "value": 605143316
          }
        },
        "a8b8c8414fda4ac1a790ea6735929f83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa4e4dcf437747508acbf72cb7216ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968ce975bbf14083b8d8f4c4be91c18c",
            "placeholder": "​",
            "style": "IPY_MODEL_5edb5938649c41669c487249ab12f919",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "bd70f0d9db9242319348b78ed0b39506": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca270fdf703847b8997837d20f68ad82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd70f0d9db9242319348b78ed0b39506",
            "placeholder": "​",
            "style": "IPY_MODEL_64bd0e65b07243458ee099a691d276f6",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "d6a74027452b4a01bedc6c0cb8688de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea438ecd51a24ee5aa86d0b68f3bf051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c32f61d6bf64717bc08869fdb7cc721",
            "placeholder": "​",
            "style": "IPY_MODEL_0692432419e84411838cecb988d23f1f",
            "value": " 605M/605M [00:06&lt;00:00, 93.5MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
